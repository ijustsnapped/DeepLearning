# configs/dino_cross_entropy_w_sampler.yaml

# --- Experiment Setup ---
experiment_setup:
  experiment_name: "dinov2_vit_s14_ft_nometa"
  seed: 21
  device: "cuda"
  TQDM_NCOLS: 100

# --- Model Configuration ---
model:
  MODEL_TYPE: "dino_vit_s14"                # DINOv2 ViT‐S/14 backbone + 1‐layer head
  num_classes: 8                            # Number of target classes
  force_reload_hub: False                   # Set True to force redownload

# --- Training Loop Parameters (Full Fine‐Tuning Phase Only) ---
training:
  num_epochs: 24                            # 24 epochs of full fine‐tuning 
  batch_size: 64                            # Batch size per GPU (adjust to memory capacity)

  optimizer:
    type: "AdamW"                           # AdamW is standard for ViT fine‐tuning 
    lr: 0.0001                              # Base LR = 1×10⁻⁴ for backbone + head 
    weight_decay: 0.05                      # Weight decay ≈ 0.05 to regularize the Transformer 

  scheduler:
    type: "CosineAnnealingLR"               # Cosine‐annealing decay over 24 epochs 
    t_max: 24
    min_lr: 1e-06

  warmup_epochs: 3                          # Warm up LR from 0 → 1×10⁻⁴ over first 3 epochs 

  amp_enabled: True                         # Mixed precision training for speed and memory 
  accum_steps: 1                            # No gradient accumulation
  val_interval: 1                           # Validate every epoch
  model_selection_metric: "mean_optimal_sensitivity"
  save_optimal_thresholds: True
  early_stopping_patience: 5                # Patience = 5 epochs before stopping 

  ema_decay: 0.999                          # EMA decay for model weights 
  use_ema_for_val: True                     # Use EMA weights during validation

  loss:
    type: "cross_entropy"                   # CrossEntropyLoss with label smoothing
    label_smoothing: 0.1                    # Label smoothing = 0.1 

  mixup_alpha: 1.0                          # Mixup alpha 
  cutmix_alpha: 1.0                         # CutMix alpha 
  mixup_prob: 0.5                           # Apply Mixup/CutMix 50% of the time

  drw_schedule_epochs: []                   # Not used for CrossEntropy
  pauc_max_fpr: 0.2                         # Partial AUC threshold

# --- Data Handling & Augmentations ---
data:
  num_workers: 8                            # Number of CPU workers for data loading
  prefetch_factor: 4                        # Prefetch factor per worker
  sampler:
    type: "default"                         # No class‐balanced sampler needed here

  cpu_augmentations:
    resize: 256                             # Resize shorter edge to 256
    crop_size: 224                          # Final crop size = 224 
    norm_mean: [0.485, 0.456, 0.406]         # Standard ImageNet normalization 
    norm_std: [0.229, 0.224, 0.225]

    train:
      random_resized_crop:                   # RandomResizedCrop parameters
        scale: [0.08, 1.0]
        ratio: [0.75, 1.333]

      random_horizontal_flip_p: 0.5          # Horizontal flip with p = 0.5 
      random_grayscale_p: 0.2                # Convert to grayscale with p = 0.2 

      color_jitter:
        brightness: 0.4
        contrast: 0.4
        saturation: 0.4
        hue: 0.1
        p: 0.8                               # Apply ColorJitter with p = 0.8 

      rand_augment:                          # RandAugment (N = 2, M = 9) with p = 0.5 
        N: 2
        M: 9
        p: 0.5

      mixup:
        alpha: 1.0
        prob: 0.5

      cutmix:
        alpha: 1.0
        prob: 0.5

      random_erasing:
        p: 0.25                             # RandomErasing with p = 0.25 
        scale: [0.02, 0.33]
        ratio: [0.3, 3.3]

  gpu_augmentations:
    enable: False
    pipeline: []

# --- Paths Configuration ---
paths:
  labels_csv: "../splits/training/labels.csv"
  train_root: "../splits/training"
  log_dir: "../outputs/tensorboard/dinov2_no_meta"
  ckpt_dir: "../outputs/checkpoints/dinov2_no_meta"

# --- PyTorch Compile (optional) ---
torch_compile:
  enable: False
  mode: "default"

# --- TensorBoard Logging ---
tensorboard_logging:
  enable: True
  log_interval_batches_train: 40
  log_interval_batches_val: 10
  log_epoch_summary: True
  log_lr: True
  log_throughput: True
  log_gpu_time_epoch: True

  image_logging:
    enable: True
    num_samples: 4
    denormalize: True
    log_at_epochs: [3, 12, 23]             # Example epochs for logging
    gradcam_layer: "blocks.10"             # Adjust layer name for DINOv2 ViT‐S/14 
    log_train_input: False
    log_val_input: False

  profiler:
    enable: False
    profile_epoch: 1
    wait_steps: 5
    warmup_steps: 5
    active_steps: 10
    repeat_cycles: 0
    record_shapes: True
    profile_memory: True
    with_stack: True
    sort_by: "self_cuda_time_total"
    row_limit: 20
    export_chrome_trace_manual: False

  memory_logging:
    enable: True
    log_interval_batches: 100
    log_epoch_summary: True
