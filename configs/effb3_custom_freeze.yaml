# src/models/effb3_advanced_imbalance.yaml
# Configuration for EfficientNet-B3 with advanced imbalance handling techniques

# -------------------------------------------------
# Experiment & Model Core Settings
# -------------------------------------------------
MODEL_TYPE: "efficientnet_b3"    # Backbone key for models.getModel
NUM_CLASSES: 9                   # Number of output classes (Update if different)
SEED: 42                         # For reproducibility
DETERMINISTIC_TRAINING: False    # Set to True for full reproducibility (slower), False for speed

# -------------------------------------------------
# Hardware & Workers
# -------------------------------------------------
DEVICE: "cuda"                   # "cuda" or "cpu"
NUM_WORKERS: 8                   # Number of CPU workers for DataLoader
PREFETCH_FACTOR: 4               # For DataLoader
USE_AMP: True                    # Automatic Mixed Precision (recommended for NVIDIA GPUs)
TORCH_COMPILE_MODE: "reduce-overhead" # "default", "reduce-overhead", "max-autotune", or null/remove to disable (PyTorch 2.x+)

# -------------------------------------------------
# Training Hyperparameters
# -------------------------------------------------
BATCH_SIZE: 24                   # EfficientNet-B3 is larger than B0; you might need to reduce BATCH_SIZE from 32
                                 # Adjust based on your GPU memory (e.g., 16, 24, 32)
NUM_EPOCHS: 50                   # Total number of training epochs
FREEZE_EPOCHS: 0                 # Head-only training for first N epochs (0 to disable)
ACCUM_STEPS: 1                   # Gradient accumulation steps

# Optimizer
LEARNING_RATE: 2e-4              # Initial learning rate (may need tuning for B3, slightly lower than B0's 3e-4)
BACKBONE_LR_MULT: 0.1            # Multiplier for backbone LR if FREEZE_EPOCHS > 0 and then unfreeze
WEIGHT_DECAY: 1e-4

# EMA
EMA_DECAY: 0.999

# -------------------------------------------------
# Scheduler & Stopping
# -------------------------------------------------
SCHEDULER: "CosineAnnealingLR"   # "StepLR" or "CosineAnnealingLR"
# For StepLR:
STEP_SIZE: 10
GAMMA: 0.5
# For CosineAnnealingLR:
T_MAX: 50                        # Should match NUM_EPOCHS for one full cycle
MIN_LR: 1e-6                     # Minimum learning rate for CosineAnnealingLR

# Validation & Early Stopping
VALIDATE_EVERY_N_EPOCHS: 1       # Run validation every N epochs (e.g., 1, 2, 4)
VALIDATION_METRIC: "auc"         # "auc" or "f1_optimal_thresholds"
EARLY_STOPPING_PATIENCE: 10      # Patience in terms of *training epochs* (script adjusts for val frequency)

# F1 Optimal Threshold Search (if VALIDATION_METRIC is "f1_optimal_thresholds")
F1_OPTIMIZATION_THRESHOLD_MIN: 0.05
F1_OPTIMIZATION_THRESHOLD_MAX: 0.95
F1_OPTIMIZATION_THRESHOLD_STEPS: 19

# -------------------------------------------------
# Data Sampling & Imbalance Handling
# -------------------------------------------------
DATASET_SAMPLING: "instance_balanced" # "instance_balanced" or "class_balanced_sqrt"
LOSS_TYPE: "ce"                       # "ce" or "ldam_drw_ce"
# For LDAM-DRW (if LOSS_TYPE is "ldam_drw_ce")
DRW_START_EPOCH: 35                   # Epoch to start deferred re-weighting (e.g., 0.7 * NUM_EPOCHS)
                                      # Set to NUM_EPOCHS or more to effectively use only sampler with CE if beta > 0
EFFECTIVE_NUM_BETA: 0.999             # Beta for effective number. 0.0 means no re-weighting by En. (0.9 to 0.9999 common)

# -------------------------------------------------
# Advanced Augmentations (MixUp/CutMix)
# -------------------------------------------------
MIXUP_ALPHA: 0.0                 # Alpha for MixUp (0 to disable)
CUTMIX_ALPHA: 0.0                # Alpha for CutMix (0 to disable)
CLASS_AWARE_MIXUP_CUTMIX: False  # True to enable class-aware aspect
RARE_CLASS_DEF_PERCENTILE: 0.25  # Defines "rare" classes for class-aware sampling (bottom X% by count)

# -------------------------------------------------
# Class Definition (Order MUST match output layer if used for label mapping)
# -------------------------------------------------
CLASS_NAMES: # Update with your actual class names in the desired output order
  - MEL
  - NV
  - BCC
  - AK
  - BKL
  - DF
  - VASC
  - SCC
  - UNK

# -------------------------------------------------
# Augmentation & Preprocessing
# -------------------------------------------------
# Standard input size for EfficientNet-B3 is 300x300
RESIZE: [320, 320]               # Resize slightly larger than crop
CROP_SIZE: 300                   # Crop to EffNet-B3 standard input size

# Basic Augmentations
HFLIP_P: 0.5
ROTATE_DEGREES: 15

# Color Jitter
COLOR_JITTER: [0.2, 0.2, 0.2, 0.1] # brightness, contrast, saturation, hue
CJ_P: 0.5                          # Probability of applying ColorJitter

# RandAugment
RAND_AUG_N: 2
RAND_AUG_M: 9

# -------------------------------------------------
# Normalization (Standard ImageNet)
# -------------------------------------------------
NORM_MEAN: [0.485, 0.456, 0.406]
NORM_STD: [0.229, 0.224, 0.225]

# -------------------------------------------------
# Model Specific (Optional, for organizing params passed to model factory)
# -------------------------------------------------
# PRETRAINED: True # This is usually handled by models.py, but can be an override
HEAD_KEYWORDS: ["_fc", "fc", "classifier", "head"] # For splitting backbone/head params, EffNet uses _fc